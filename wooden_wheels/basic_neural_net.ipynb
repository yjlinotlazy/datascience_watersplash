{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a basic neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a badly drawn but still valid fully connected neural net with 3 features, 1 hidden layer, and 1-dimensional output.\n",
    "```\n",
    "x1\n",
    "  |\\\n",
    "  | \\\n",
    "  |  \\\n",
    "  |   \\\n",
    "  | |--node1\n",
    "  | |   / \\\n",
    "  | |  /   \\\n",
    "x2--+-/     y\n",
    "  | | \\    /\n",
    "  | |  \\  /\n",
    "  |-+- node2\n",
    "    | /\n",
    "    |/ \n",
    "    /\n",
    "   /\n",
    "x3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "Here's the formula for above network, given the feature values of a single data point,\n",
    "\n",
    "First layer:\n",
    "    \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "node1 \\\\\n",
    "node2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma(\n",
    "\\begin{bmatrix} \n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Second layer\n",
    "\n",
    "$$\n",
    "y = \n",
    "\\sigma(\n",
    "\\begin{bmatrix}\n",
    "\\beta_1 & \\beta_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "node1 \\\\\n",
    "node2 \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "d\n",
    ")\n",
    "$$\n",
    "\n",
    "$w$ and $\\beta$ are the weights and $b, d$ are the bias terms. The simoid function, $\\sigma$, is used as activation function for both layers\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data vs prediction\n",
    "\n",
    "Suppose there are n samples, to go from those $x$ values to expected $y$ values, include all samples and slightly rearrange above formula to\n",
    "\n",
    "First layer\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "node_{11} & node_{12} \\\\\n",
    "node_{21} & node_{22} \\\\\n",
    "... \\\\\n",
    "node_{n1} & node_{n2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma(\n",
    "\\begin{bmatrix} \n",
    "x_{11} & x_{12} & x_{13} \\\\\n",
    "x_{21} & x_{22} & x_{23} \\\\\n",
    "... \\\\\n",
    "x_{n1} & x_{n2} & x_{n3}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 \\\\\n",
    "b_1 & b_2 \\\\\n",
    "...\\\\\n",
    "b_1 & b_2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "= \\sigma(XW^T + B)\n",
    "$$\n",
    "\n",
    "Output layer\n",
    "\n",
    "$$\n",
    "\\hat{Y} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{y_1} \\\\\n",
    "\\hat{y_2} \\\\\n",
    "... \\\\\n",
    "\\hat{y_n}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sigma(\n",
    "\\begin{bmatrix} \n",
    "node_{11} & node_{12} \\\\\n",
    "node_{21} & node_{22} \\\\\n",
    "... \\\\\n",
    "node_{n1} & node_{n2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "d \\\\\n",
    "d \\\\\n",
    "..\\\\\n",
    "d\n",
    "\\end{bmatrix}\n",
    ")\n",
    "= \\sigma(Nodes\\times V^T + D)\n",
    "$$\n",
    "\n",
    "However, in this imperfect world, there's usually a gap between actual and expected value.\n",
    "Instead of using sum-of-sqaures error, let's introduce another cost function suitable for\n",
    "using the sigmoid function for probability estimations.\n",
    "\n",
    "The goal of model training is to minimize above loss function. We want the weights and biases\n",
    "associated with minimal loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find best set of parameters?\n",
    "\n",
    "Unlike linear regression, there's no algebraic solution to find optimal coefficients. We\n",
    "need to iteratively \"guess\" a set of weights, calculate the errors, and infer from the errors\n",
    "how we should modify the weights towards a better direction (lower errors).\n",
    "\n",
    "N x (update weights -> calculate errors -> find out how to update weights)\n",
    "\n",
    "The practice of using errors at output layer to update weights from the very first\n",
    "hidden layer is called **back propagation**. How do we use errors? Would be nice if\n",
    "someone can tell me something like \"if you change $w_1$ by 0.2, then you can reduce the\n",
    "errors by 0.3\". Keep in mind that while this tells us the _direction_ of the next move, it\n",
    "doesn't tell us how much to move before we reach a minimum.\n",
    "\n",
    "To get such direction, we want to know the derivative of loss function with respect to\n",
    "each weight and bias. The prerequisite is that the loss function must be differentiable\n",
    "w.r.t all weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### An"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:th]",
   "language": "python",
   "name": "conda-env-th-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
