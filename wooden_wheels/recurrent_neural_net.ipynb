{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442baf86-b098-48be-b917-9e1364cdaf69",
   "metadata": {},
   "source": [
    "# Recurrent Neural Net From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24d34a-4532-402b-a879-8ad5d6f54866",
   "metadata": {},
   "source": [
    "I will skip the reasoning behind why we need RNN. There's plenty on the internet. I'll focus on learning the technical side, and hope that the answers to \"why\"s will come naturally. This is the same principal as other notebooks in my \"from scratch\" series. This is heavily based on the basic neural net, so read that one first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975f819-9178-49bd-973e-0b66077089cb",
   "metadata": {},
   "source": [
    "Recall that in a neural network, we have:\n",
    "\n",
    "![nn](imgs/nn.png)\n",
    "\n",
    "For recurrent neural network, we carry over info from the previous data point in the sequence, by incorporating the values of the hidden layer, with its own weight $u$.\n",
    "\n",
    "![nn](imgs/recurrent_neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed60bf-8ff4-497f-a940-385ac635a0f6",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "\n",
    "Rephrasing the diagram, we have the feed forward process as, for each time step $t$,\n",
    " * $s_{h,t} = x_t W + a_{h,t-1}U + b_h$\n",
    " * $a_{h,t} = \\sigma_h(s_{h,t})$\n",
    " * $s_{o,t} = a_{h,t}V +b_o$\n",
    " * $y_t = \\sigma_o(s_{o,t})$\n",
    "\n",
    "h means hidden layer, o means output layer, s means weighted sum, a means activated value. $\\sigma$ means the activation function. For the activation functions, we use softmax at the output layer and tanh at the hidden layer.\n",
    "\n",
    "$$\\large\n",
    "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "It has the convenient property that\n",
    "\n",
    "$$ \\large\n",
    "(tanh(x))' = 1-(tanh(x))^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91459bbd-d455-44a8-a9f9-4cf45828c516",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time\n",
    "* As there's dependency bewteen the previous and next steps, backpropagation in RNN goes through the entire sequence, called Backpropagation Through Time (BPTT) where we propagate the gradient not only through layers, but also through the entire sequence of data. This means we sum up the lost of the predictions of the sequece.\n",
    "\n",
    "Overall loss is\n",
    "\n",
    "$$\n",
    "L_{total} = \\sum_1^t L_t\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "L_t = -y_t log(\\hat{y_t})\n",
    "$$\n",
    "\n",
    "* Backpropagation from the output layer to the hidden layer is similar as vanila neural network since we don't add anything new between these two layers\n",
    "* Backpropagation from the hidden layer will go to both the input layer and the hidden layer of the previous data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28781a38-8191-4737-861e-4088a12fdc17",
   "metadata": {},
   "source": [
    "### Weights from the output layer to the hidden layer\n",
    "\n",
    "From the math of basic neural net, we know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae962a44-42e7-49f8-81b4-58d2ce7fc01c",
   "metadata": {},
   "source": [
    "$$ \\large\n",
    "\\frac{\\partial L_t}{\\partial V} = (\\hat{y_t} - y_t) \\cdot a_{h,t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e45d3c-c97e-44ea-af87-29fa3a0c9340",
   "metadata": {},
   "source": [
    "Replace $a_h$ with 1 for the bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2712a-b9a1-46b6-ac02-8acca1198981",
   "metadata": {},
   "source": [
    "### Weights from the hidden to the input layer and previous data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d826a-775d-40cd-b8cb-b3ce11dc1f99",
   "metadata": {},
   "source": [
    "From above setup, we can see that $s_{h,t}$ is an important term as it's a function of $W, U, a_{h,t-1}$. In backpropagation, the gradient will flow from $s_{h,t}$ to those components like the following, based on the formula in the feedforward section above:\n",
    "\n",
    "$$\\large\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_t}{\\partial W} &= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot x_t \\\\\n",
    "\\frac{\\partial L_t}{\\partial U} &= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot a_{h,t-1} \\\\\n",
    "\\frac{\\partial L_t}{\\partial b_h} &= \\frac{\\partial L_t}{\\partial s_{h,t}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To get $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$, we have\n",
    "\n",
    "$$\\large\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_t}{\\partial s_{h,t}} & = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial a_{h,t}}{\\partial s_{h,t}} \\\\\n",
    "& = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial \\sigma_h(s_{h,t})}{\\partial s_{h,t}} \\\\\n",
    "& = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot (1-(tanh(s_{h,t}))^2) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "All we have left to derive is $\\Large \\frac{\\partial L_t}{\\partial a_{h,t}}$. As $\\large a_{h,t}$ contributes to $L_{t+1}$, the gradient from $L_{t+1}$ also goes to $\\large a_{h,t}$. Therefore\n",
    "\n",
    "$$\\large\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_t}{\\partial a_{h,t}} &= \\frac{\\partial L_t}{\\partial s_{o,t}} \\cdot \\frac{\\partial s_{o,t}}{\\partial a_{h,t}} + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot \\frac{\\partial s_{h,t+1}}{\\partial a_{h,t}} \\\\\n",
    "&= (\\hat{y_t} - y_t)\\cdot V + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot U\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be416e9-0aac-4c33-a22b-1bd1d71d6d64",
   "metadata": {},
   "source": [
    "## Problem to solve\n",
    "\n",
    "We'll use RNN to solve a toy problem of predicting the next number in mod 3. 1 is next to 0, 2 is next to 1, 0 is next to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d36aaf5-68f7-4b57-9bd2-f8bb1dbf8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  10110101011110211012000212200102101112002100120200\n",
      "output: 21221212122221022120111020011210212220110211201011\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# length of the sequence is the length of the problem. Keep it not too big for calculation, but not too small to be boring.\n",
    "length = 50\n",
    "\n",
    "x_num = [np.random.randint(0, 3) for i in range(length)]\n",
    "y_num = [(x+1) % 3 for x in x_num]\n",
    "x_str = ''.join([str(i) for i in x_num])\n",
    "y_str = ''.join([str(i) for i in y_num])\n",
    "print('input: ', x_str)\n",
    "print('output:', y_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc2176-a315-498f-ac97-f5b41945bd61",
   "metadata": {},
   "source": [
    "Repeat to make a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f442edb0-0d89-4848-9205-1b2ee9b6959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_label(y, vocab=3):\n",
    "    # turn a single digit label to multiclass labels\n",
    "    labels = np.zeros((len(y), vocab), dtype=int)\n",
    "    for i, yy in enumerate(y):\n",
    "        labels[i, yy] = 1\n",
    "    return labels\n",
    "\n",
    "def make_pairs(length=10, vocab=3):\n",
    "    x_num = [np.random.randint(0, vocab) for i in range(length)]\n",
    "    y_num = [(x+1) % vocab for x in x_num]\n",
    "    # One hot encode the input and output\n",
    "    x = num_to_label(x_num)\n",
    "    y = num_to_label(y_num)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8f5f7c39-b687-4f39-bfe9-e7ed328ace0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "x, y = [], []\n",
    "for i in range(size):\n",
    "    xx, yy = make_pairs()\n",
    "    x.append(xx)\n",
    "    y.append(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e427e29-eb40-4856-b04e-351434725fb2",
   "metadata": {},
   "source": [
    "Different from basic neural net, RNN does feed forward and backpropagation for the entire sequence, not at each step independently. First let's layout the components we need. This should give a rough breakdown to help writing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc9576d-b0a2-45e0-aedf-394b3f1b2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self):\n",
    "        # initialize a random set of weigths and biases\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        # this is the feed forward process\n",
    "        pass\n",
    "\n",
    "    def backpropagation(self):\n",
    "        # one round of backpropagation through the entire sequence\n",
    "        pass\n",
    "\n",
    "    def train(self, epochs):\n",
    "        pass\n",
    "\n",
    "    def total_loss(self):\n",
    "        pass\n",
    "\n",
    "    def loss(self, t):\n",
    "        return \n",
    "\n",
    "    def tanh(x):\n",
    "        pass\n",
    "\n",
    "    def softmax(x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74efe16-9bca-45ef-9129-ba9a540718a6",
   "metadata": {},
   "source": [
    "Then fill in the blanks. We'll first enable feed forward functions. In the init function, we not only need to initialize the parameters, but also reserve vectors to memorize all the calculations, as we'll rely on them in both feed forward and BTT. As we see above, terms like $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$ are used multiple times and in different epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c7071948-ab7f-49f3-9329-36ea7f3b4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNN1():\n",
    "    def __init__(self, n_nodes=10, vocab_size=3):\n",
    "        # initialize a random set of weigths and biases\n",
    "        self.n_nodes = n_nodes\n",
    "        self.w = np.random.randn(n_nodes, vocab_size)\n",
    "        self.u = np.random.randn(n_nodes, n_nodes)\n",
    "        self.v = np.random.randn(vocab_size,n_nodes)\n",
    "        self.b_h = np.random.randn(n_nodes)\n",
    "        self.b_o = np.random.randn(vocab_size)\n",
    "        self.s_h = []\n",
    "        self.a_h = []\n",
    "        self.s_o = []\n",
    "        self.y_hat = []\n",
    "\n",
    "    def predict(self, x):\n",
    "        return [self.predict_single(xx) for xx in x]\n",
    "\n",
    "    def predict_scores(self, x):\n",
    "        return [self.feedforward(xx) for xx in x]\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        scores = self.feedforward(x)\n",
    "        return self.score_to_num(scores)\n",
    "\n",
    "    def reset_mem(self):\n",
    "        self.s_h = []\n",
    "        self.a_h = []\n",
    "        self.s_o = []\n",
    "        self.y_hat = []\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        self.reset_mem()\n",
    "        # this is the feed forward process\n",
    "        for t in range(len(x)):\n",
    "            # this follows straight from the formula\n",
    "            a_ht_prev = np.zeros(self.n_nodes) if t == 0 else self.a_h[t-1]\n",
    "            s_ht = x[t] @ self.w.T + a_ht_prev @ self.u + self.b_h\n",
    "            a_ht = self.tanh(s_ht)\n",
    "            s_ot = a_ht @ self.v.T + self.b_o\n",
    "            y_t = self.softmax(s_ot)\n",
    "\n",
    "            # memorize all above\n",
    "            self.s_h.append(s_ht)\n",
    "            self.a_h.append(a_ht)\n",
    "            self.s_o.append(s_ot)\n",
    "            self.y_hat.append(y_t)\n",
    "        return self.y_hat\n",
    "\n",
    "    def total_loss(self, y, y_hat):\n",
    "        return sum([self.loss(t, y, y_hat) for t in range(len(y))])\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(t, y, y_hat):\n",
    "        if (y[t] == y_hat[t]).all():\n",
    "            return 0\n",
    "        return np.sum(-y[t]*np.log(y_hat[t]))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        x = np.array(x, dtype=np.float128)\n",
    "        # to avoid overflow due to super large x, we subtract the max\n",
    "        xx = x - np.max(x)\n",
    "        exp = np.exp(xx)\n",
    "        return exp / exp.sum(keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def score_to_num(scores):\n",
    "        indices = [np.argmax(p) for p in scores]\n",
    "        return np.array(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42884555-1313-4fc7-b9c6-c62262475813",
   "metadata": {},
   "source": [
    "Run some random numbers to make sure matrix dimensions are good. We have written functions to\n",
    " * Make predictinos by feed forward, using randomly generated parameters\n",
    " * Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3b418420-93d8-498d-8ba2-381cf6bed9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1 = RNN1()\n",
    "pred_scores = rnn1.predict_scores(x)\n",
    "pred = rnn1.predict(x)\n",
    "loss = rnn1.total_loss(y, pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "29a79b88-7927-412c-8c13-f5236ac3533c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([2, 0, 0, 0, 1, 2, 0, 2, 0, 0]),\n",
       "  array([0, 0, 2, 0, 2, 1, 0, 0, 1, 2]),\n",
       "  array([0, 0, 2, 0, 0, 0, 1, 0, 0, 2]),\n",
       "  array([0, 0, 2, 0, 2, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 2, 0, 2, 2, 0, 0, 1, 0]),\n",
       "  array([2, 0, 0, 0, 1, 2, 0, 2, 0, 2]),\n",
       "  array([0, 0, 2, 2, 0, 0, 2, 1, 0, 0]),\n",
       "  array([0, 0, 2, 0, 2, 2, 0, 2, 2, 0]),\n",
       "  array([0, 0, 0, 1, 2, 0, 0, 2, 1, 0]),\n",
       "  array([0, 0, 0, 0, 2, 0, 2, 0, 0, 1])],\n",
       " np.longdouble('206.90670911142617447'))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cf80d188-6bd2-4216-8268-8abb053eccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0 = rnn1.total_loss(y[0], y[0])\n",
    "loss1 = rnn1.total_loss(pred_scores[0], pred_scores[0])\n",
    "assert loss0 == 0\n",
    "assert loss1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add289c-fdb2-4d5c-a985-bcc4e8445d1d",
   "metadata": {},
   "source": [
    "Move ahead to BPTT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b3e32fbf-2eea-4938-acf9-a083e6879451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN2(RNN1):\n",
    "    def __init__(self, learning_rate=0.0001, n_nodes=10, vocab_size=3):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.delta_l_over_s_h = []\n",
    "\n",
    "    def backpropagation(self, x, y, y_hat):\n",
    "        # BPTT for a single sequence of sample. This function\n",
    "        # returns the deltas of W, V, U, b_h, and b_o\n",
    "        T = len(y)\n",
    "        du = np.zeros(self.u.shape)\n",
    "        dv = np.zeros(self.v.shape)\n",
    "        dw = np.zeros(self.w.shape)\n",
    "        dbh = np.zeros(self.b_h.shape)\n",
    "        dbo = np.zeros(self.b_o.shape)\n",
    "        # btt goes backwards, from time t to time 0\n",
    "        for t in reversed(range(T)):\n",
    "            # one round of backpropagation at time t\n",
    "            # output layer to hidden layer.\n",
    "            # dLt/dV = (y_hat_t - y_t) * a_ht\n",
    "            # dLt/dbo = (y_hat_t - y_t)\n",
    "            test = self.a_h[t][:, np.newaxis]\n",
    "            dbo_t = (y_hat[t] - y[t].T)\n",
    "            dv_t = dbo_t[:, np.newaxis] @ test.T\n",
    "            dv += dv_t\n",
    "            dbo += dbo_t\n",
    "\n",
    "            # hidden layer to input layer\n",
    "            da_ht = (y_hat[)\n",
    "            ds_ht = da_ht @ (1-self.a_hh[t] ** 2)\n",
    "            dw_t = ds_ht @ x[t]\n",
    "            \n",
    "        return dw, dv, du, dbh, dbo\n",
    "\n",
    "    def train(self, epochs, x, y, verbose=False):\n",
    "        for i in range(epochs):\n",
    "            for j in range(len(x)):\n",
    "                # feed forward\n",
    "                y_hat = self.feedforward(x[j])\n",
    "                # calculate deltas\n",
    "                dW, dV, dU, dbh, dbo = self.backpropagation(x[j], y[j], y_hat)\n",
    "                # update weights accordingly\n",
    "                self.u -= self.learning_rate * dU\n",
    "                self.w -= self.learning_rate * dW\n",
    "                self.v -= self.learning_rate * dV\n",
    "                self.b_h -= self.learning_rate * dbh\n",
    "                self.b_o -= self.learning_rate * dbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "eae920d9-2c75-4832-994d-9c459f11fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2 = RNN2()\n",
    "rnn2.train(1, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6bb852ae-b4bb-4326-9c0b-9932802bb3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 3]]), (3, 1), (1, 3))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])[:, np.newaxis]\n",
    "a.T, a.shape, a.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "83380b2f-c662-4e2f-953d-4587a9530eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[200]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m a = [\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a254063-f74f-49d4-8a72-cd1bce8ff4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
