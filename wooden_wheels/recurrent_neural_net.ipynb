{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442baf86-b098-48be-b917-9e1364cdaf69",
   "metadata": {},
   "source": [
    "# Recurrent Neural Net From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24d34a-4532-402b-a879-8ad5d6f54866",
   "metadata": {},
   "source": [
    "I will skip the reasoning behind why we need RNN. There's plenty on the internet. I'll focus on learning the technical side, and hope that the answers to \"why\"s will come naturally. This is the same principal as other notebooks in my \"from scratch\" series. This is heavily based on the basic neural net, so read that one first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975f819-9178-49bd-973e-0b66077089cb",
   "metadata": {},
   "source": [
    "Recall that in a neural network, we have:\n",
    "\n",
    "![nn](imgs/nn.png)\n",
    "\n",
    "For recurrent neural network, we carry over info from the previous data point in the sequence, by incorporating the values of the hidden layer, with its own weight $u$.\n",
    "\n",
    "![nn](imgs/recurrent_neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed60bf-8ff4-497f-a940-385ac635a0f6",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "\n",
    "Rephrasing the diagram, we have the feed forward process as, for each time step $t$,\n",
    " * $s_{h,t} = x_t W + a_{h,t-1}U + b_h$\n",
    " * $a_{h,t} = \\sigma_h(s_{h,t})$\n",
    " * $s_{o,t} = a_{h,t}V +b_o$\n",
    " * $y_t = \\sigma_o(s_{o,t})$\n",
    "\n",
    "h means hidden layer, o means output layer, s means weighted sum, a means activated value. $\\sigma$ means the activation function. For the activation functions, we use softmax at the output layer and tanh at the hidden layer.\n",
    "\n",
    "$$\\large\n",
    "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "It has the convenient property that\n",
    "\n",
    "$$ \\large\n",
    "(tanh(x))' = 1-(tanh(x))^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91459bbd-d455-44a8-a9f9-4cf45828c516",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time\n",
    "* As there's dependency bewteen the previous and next steps, backpropagation in RNN goes through the entire sequence, called Backpropagation Through Time (BPTT) where we propagate the gradient not only through layers, but also through the entire sequence of data. This means we sum up the lost of the predictions of the sequece.\n",
    "\n",
    "Overall loss is\n",
    "\n",
    "$$\n",
    "L_{total} = \\sum_1^t L_t\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "L_t = -y_t log(\\hat{y_t})\n",
    "$$\n",
    "\n",
    "* Backpropagation from the output layer to the hidden layer is similar as vanila neural network since we don't add anything new between these two layers\n",
    "* Backpropagation from the hidden layer will go to both the input layer and the hidden layer of the previous data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28781a38-8191-4737-861e-4088a12fdc17",
   "metadata": {},
   "source": [
    "### Weights from the output layer to the hidden layer\n",
    "\n",
    "From the math of basic neural net, we know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae962a44-42e7-49f8-81b4-58d2ce7fc01c",
   "metadata": {},
   "source": [
    "$$ \\large\n",
    "\\frac{\\partial L_t}{\\partial V} = (\\hat{y_t} - y_t) \\cdot a_{h,t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e45d3c-c97e-44ea-af87-29fa3a0c9340",
   "metadata": {},
   "source": [
    "Replace $a_h$ with 1 for the bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2712a-b9a1-46b6-ac02-8acca1198981",
   "metadata": {},
   "source": [
    "### Weights from the hidden to the input layer and previous data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d826a-775d-40cd-b8cb-b3ce11dc1f99",
   "metadata": {},
   "source": [
    "From above setup, we can see that $s_{h,t}$ is an important term as it's a function of $W, U, a_{h,t-1}$. In backpropagation, the gradient will flow from $s_{h,t}$ to those components like the following, based on the formula in the feedforward section above:\n",
    "\n",
    "$$\\large\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_t}{\\partial W} &= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot x_t \\\\\n",
    "\\frac{\\partial L_t}{\\partial U} &= \\frac{\\partial L_t}{\\partial s_{h,t}} \\cdot a_{h,t-1} \\\\\n",
    "\\frac{\\partial L_t}{\\partial b_h} &= \\frac{\\partial L_t}{\\partial s_{h,t}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To get $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$, we have\n",
    "\n",
    "$$\\large\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_t}{\\partial s_{h,t}} & = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial a_{h,t}}{\\partial s_{h,t}} \\\\\n",
    "& = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot \\frac{\\partial \\sigma_h(s_{h,t})}{\\partial s_{h,t}} \\\\\n",
    "& = \\frac{\\partial L_t}{\\partial a_{h,t}}\\cdot (1-(tanh(s_{h,t}))^2) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "All we have left to derive is $\\Large \\frac{\\partial L_t}{\\partial a_{h,t}}$. As $\\large a_{h,t}$ contributes to $L_{t+1}$, the gradient from $L_{t+1}$ also goes to $\\large a_{h,t}$. Therefore\n",
    "\n",
    "$$\\large\n",
    "\\begin{align}\n",
    "\\frac{\\partial L_t}{\\partial a_{h,t}} &= \\frac{\\partial L_t}{\\partial s_{o,t}} \\cdot \\frac{\\partial s_{o,t}}{\\partial a_{h,t}} + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot \\frac{\\partial s_{h,t+1}}{\\partial a_{h,t}} \\\\\n",
    "&= (\\hat{y_t} - y_t)\\cdot V + \\frac{\\partial L_{t+1}}{\\partial s_{h,t+1}} \\cdot U\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be416e9-0aac-4c33-a22b-1bd1d71d6d64",
   "metadata": {},
   "source": [
    "## Problem to solve\n",
    "\n",
    "We'll use RNN to solve a toy problem of predicting the next number in mod 3. 1 is next to 0, 2 is next to 1, 0 is next to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d36aaf5-68f7-4b57-9bd2-f8bb1dbf8ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  11220020221211221211112201122011202120001210122112\n",
      "output: 22001101002022002022220012200122010201112021200220\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# length of the sequence is the length of the problem. Keep it not too big for calculation, but not too small to be boring.\n",
    "length = 50\n",
    "\n",
    "x_num = [np.random.randint(0, 3) for i in range(length)]\n",
    "y_num = [(x+1) % 3 for x in x_num]\n",
    "x_str = ''.join([str(i) for i in x_num])\n",
    "y_str = ''.join([str(i) for i in y_num])\n",
    "print('input: ', x_str)\n",
    "print('output:', y_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc2176-a315-498f-ac97-f5b41945bd61",
   "metadata": {},
   "source": [
    "Repeat to make a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f442edb0-0d89-4848-9205-1b2ee9b6959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_label(y, vocab=3):\n",
    "    # turn a single digit label to multiclass labels\n",
    "    labels = np.zeros((len(y), vocab), dtype=int)\n",
    "    for i, yy in enumerate(y):\n",
    "        labels[i, yy] = 1\n",
    "    return labels\n",
    "\n",
    "def make_pairs(length=10, vocab=3):\n",
    "    x_num = [np.random.randint(0, vocab) for i in range(length)]\n",
    "    y_num = [(x+1) % vocab for x in x_num]\n",
    "    # One hot encode the input and output\n",
    "    x = num_to_label(x_num)\n",
    "    y = num_to_label(y_num)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5f7c39-b687-4f39-bfe9-e7ed328ace0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "x, y = [], []\n",
    "for i in range(size):\n",
    "    xx, yy = make_pairs()\n",
    "    x.append(xx)\n",
    "    y.append(yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e427e29-eb40-4856-b04e-351434725fb2",
   "metadata": {},
   "source": [
    "Different from basic neural net, RNN does feed forward and backpropagation for the entire sequence, not at each step independently. First let's layout the components we need. This should give a rough breakdown to help writing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc9576d-b0a2-45e0-aedf-394b3f1b2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self):\n",
    "        # initialize a random set of weigths and biases\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        # this is the feed forward process\n",
    "        pass\n",
    "\n",
    "    def backpropagation(self):\n",
    "        # one round of backpropagation through the entire sequence\n",
    "        pass\n",
    "\n",
    "    def train(self, epochs):\n",
    "        pass\n",
    "\n",
    "    def total_loss(self):\n",
    "        pass\n",
    "\n",
    "    def loss(self, t):\n",
    "        return \n",
    "\n",
    "    def tanh(x):\n",
    "        pass\n",
    "\n",
    "    def softmax(x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74efe16-9bca-45ef-9129-ba9a540718a6",
   "metadata": {},
   "source": [
    "Then fill in the blanks. We'll first enable feed forward functions. In the init function, we not only need to initialize the parameters, but also reserve vectors to memorize all the calculations, as we'll rely on them in both feed forward and BTT. As we see above, terms like $\\Large \\frac{\\partial L_t}{\\partial s_{h,t}}$ are used multiple times and in different epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7071948-ab7f-49f3-9329-36ea7f3b4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNN1():\n",
    "    def __init__(self, n_nodes=10, vocab_size=3):\n",
    "        # initialize a random set of weigths and biases\n",
    "        self.n_nodes = n_nodes\n",
    "        self.w = np.random.randn(n_nodes, vocab_size)\n",
    "        self.u = np.random.randn(n_nodes, n_nodes)\n",
    "        self.v = np.random.randn(vocab_size,n_nodes)\n",
    "        self.b_h = np.random.randn(n_nodes)\n",
    "        self.b_o = np.random.randn(vocab_size)\n",
    "        self.s_h = []\n",
    "        self.a_h = []\n",
    "        self.s_o = []\n",
    "        self.y_hat = []\n",
    "\n",
    "    def predict(self, x):\n",
    "        return [self.predict_single(xx) for xx in x]\n",
    "\n",
    "    def predict_scores(self, x):\n",
    "        return [self.feedforward(xx) for xx in x]\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        scores = self.feedforward(x)\n",
    "        return self.score_to_num(scores)\n",
    "\n",
    "    def reset_mem(self):\n",
    "        self.s_h = []\n",
    "        self.a_h = []\n",
    "        self.s_o = []\n",
    "        self.y_hat = []\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        self.reset_mem()\n",
    "        # this is the feed forward process\n",
    "        for t in range(len(x)):\n",
    "            # this follows straight from the formula\n",
    "            a_ht_prev = np.zeros(self.n_nodes) if t == 0 else self.a_h[t-1]\n",
    "            s_ht = x[t] @ self.w.T + a_ht_prev @ self.u + self.b_h\n",
    "            a_ht = self.tanh(s_ht)\n",
    "            s_ot = a_ht @ self.v.T + self.b_o\n",
    "            y_t = self.softmax(s_ot)\n",
    "\n",
    "            # memorize all above\n",
    "            self.s_h.append(s_ht)\n",
    "            self.a_h.append(a_ht)\n",
    "            self.s_o.append(s_ot)\n",
    "            self.y_hat.append(y_t)\n",
    "        return self.y_hat\n",
    "\n",
    "    def total_loss(self, y, y_hat):\n",
    "        return sum([self.loss(t, y, y_hat) for t in range(len(y))])\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(t, y, y_hat):\n",
    "        if (y[t] == y_hat[t]).all():\n",
    "            return 0\n",
    "        return np.sum(-y[t]*np.log(y_hat[t]))\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        x = np.array(x, dtype=np.float128)\n",
    "        # to avoid overflow due to super large x, we subtract the max\n",
    "        xx = x - np.max(x)\n",
    "        exp = np.exp(xx)\n",
    "        return exp / exp.sum(keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def score_to_num(scores):\n",
    "        indices = [np.argmax(p) for p in scores]\n",
    "        return np.array(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42884555-1313-4fc7-b9c6-c62262475813",
   "metadata": {},
   "source": [
    "Run some random numbers to make sure matrix dimensions are good. We have written functions to\n",
    " * Make predictinos by feed forward, using randomly generated parameters\n",
    " * Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b418420-93d8-498d-8ba2-381cf6bed9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1 = RNN1()\n",
    "pred_scores = rnn1.predict_scores(x)\n",
    "pred = rnn1.predict(x)\n",
    "loss = rnn1.total_loss(y, pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29a79b88-7927-412c-8c13-f5236ac3533c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1, 0, 0, 0, 1, 0, 0, 1, 1, 0]),\n",
       "  array([2, 0, 0, 0, 1, 0, 0, 1, 2, 0]),\n",
       "  array([1, 0, 2, 1, 0, 0, 2, 0, 0, 0]),\n",
       "  array([1, 0, 2, 1, 0, 0, 0, 0, 1, 0]),\n",
       "  array([1, 0, 0, 0, 1, 2, 2, 2, 0, 0]),\n",
       "  array([2, 0, 0, 0, 0, 1, 0, 0, 1, 1]),\n",
       "  array([1, 0, 2, 2, 0, 0, 1, 0, 1, 2]),\n",
       "  array([2, 0, 0, 0, 1, 0, 0, 1, 1, 0]),\n",
       "  array([2, 0, 0, 0, 0, 0, 1, 0, 0, 2]),\n",
       "  array([1, 0, 0, 0, 1, 0, 0, 1, 1, 0])],\n",
       " np.longdouble('164.06705644911090597'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, loss, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf80d188-6bd2-4216-8268-8abb053eccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate correctness of the loss calculation\n",
    "loss0 = rnn1.total_loss(y[0], y[0])\n",
    "loss1 = rnn1.total_loss(pred_scores[0], pred_scores[0])\n",
    "assert loss0 == 0\n",
    "assert loss1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add289c-fdb2-4d5c-a985-bcc4e8445d1d",
   "metadata": {},
   "source": [
    "Move ahead to BPTT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b3e32fbf-2eea-4938-acf9-a083e6879451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN2(RNN1):\n",
    "    def __init__(self, learning_rate=0.0001, n_nodes=10, vocab_size=3):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.delta_l_over_s_h = []\n",
    "\n",
    "    def backpropagation(self, x, y, y_hat):\n",
    "        # BPTT for a single sequence of sample. This function\n",
    "        # returns the deltas of W, V, U, b_h, and b_o\n",
    "        T = len(y)\n",
    "        du = np.zeros(self.u.shape)\n",
    "        dv = np.zeros(self.v.shape)\n",
    "        dw = np.zeros(self.w.shape)\n",
    "        dbh = np.zeros(self.b_h.shape)\n",
    "        dbo = np.zeros(self.b_o.shape)\n",
    "        # btt goes backwards, from time t to time 0\n",
    "        ds_ht_plus_1 = np.zeros(self.n_nodes)\n",
    "        for t in reversed(range(T)):\n",
    "            # one round of backpropagation at time t\n",
    "            # output layer to hidden layer.\n",
    "            # dLt/dbo = (y_hat_t - y_t)\n",
    "            # dLt/dV = (y_hat_t - y_t) * a_ht = dLt/dbo * a_ht\n",
    "            dbo_t = (y_hat[t] - y[t].T)\n",
    "            dv_t = dbo_t[:, np.newaxis] @ self.a_h[t][:, np.newaxis].T\n",
    "            dbo += dbo_t\n",
    "            dv += dv_t\n",
    "\n",
    "            # hidden layer to input layer\n",
    "            da_ht = (y_hat[t] - y[t]) @ self.v + ds_ht_plus_1 @ self.u\n",
    "            ds_ht = da_ht * (1-self.a_h[t] ** 2)\n",
    "            ds_ht_plus_1 = ds_ht\n",
    "            dw_t = ds_ht[:, np.newaxis] @ x[t][:, np.newaxis].T\n",
    "\n",
    "            if t-1 >= 0:\n",
    "                a_ht_1 = self.a_h[t-1]\n",
    "            else:\n",
    "                a_ht_1 = np.zeros(self.n_nodes)\n",
    "            du_t = ds_ht @ a_ht_1\n",
    "\n",
    "            dbh_t = ds_ht\n",
    "\n",
    "            dw += dw_t\n",
    "            du += du_t\n",
    "            dbh += dbh_t\n",
    "            \n",
    "        return dw, dv, du, dbh, dbo\n",
    "\n",
    "    def train(self, epochs, x, y, verbose=False):\n",
    "        for i in range(epochs):\n",
    "            loss = 0\n",
    "            for j in range(len(x)):\n",
    "                # feed forward\n",
    "                y_hat = self.feedforward(x[j])\n",
    "                # current loss\n",
    "                loss += self.total_loss(y[j], y_hat)\n",
    "                # calculate deltas\n",
    "                dW, dV, dU, dbh, dbo = self.backpropagation(x[j], y[j], y_hat)\n",
    "                # update weights accordingly\n",
    "                self.u -= self.learning_rate * dU\n",
    "                self.w -= self.learning_rate * dW\n",
    "                self.v -= self.learning_rate * dV\n",
    "                self.b_h -= self.learning_rate * dbh\n",
    "                self.b_o -= self.learning_rate * dbo\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f\"loss at epoch {i}: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371a86b7-84e1-43b8-9675-3ecfe29b9d8c",
   "metadata": {},
   "source": [
    "Some helper function to visualize the quality of the model before and after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f97f1341-dc43-4cee-b548-14a0e70e1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2str(arr):\n",
    "    return ''.join(str(a) for a in arr)\n",
    "\n",
    "def evaluate_model(model, x, y_true):\n",
    "    true_pretty = [array2str(RNN2.score_to_num(yy)) for yy in y_true]\n",
    "    y_pred = model.predict(x)\n",
    "    pred_pretty = [array2str(yy) for yy in y_pred]\n",
    "    y_true_total = ''.join(true_pretty)\n",
    "    y_pred_total = ''.join(pred_pretty)\n",
    "    overlap = 0\n",
    "    for i in range(len(y_true_total)):\n",
    "        if y_true_total[i] == y_pred_total[i]:\n",
    "            overlap += 1\n",
    "    print(\"actual:\", true_pretty)\n",
    "    print(\"predicted:\", pred_pretty)\n",
    "    print(\"overlap:\", overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2ec4e3c9-fe08-4fe0-a1e9-f91c66e0db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: ['0022210110', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201220', '0002112100']\n",
      "predicted: ['1120020111', '0111110101', '1012111110', '1122021011', '1111212111', '0111110111', '1122111210', '0111210111', '0111011101', '1120112211']\n",
      "overlap: 30\n"
     ]
    }
   ],
   "source": [
    "rnn2 = RNN2(learning_rate=0.0001, n_nodes=100)\n",
    "\n",
    "evaluate_model(rnn2, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "eae920d9-2c75-4832-994d-9c459f11fccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0: 202.7836139814427\n",
      "loss at epoch 100: 110.27131099270905\n",
      "loss at epoch 200: 83.10060656394155\n",
      "loss at epoch 300: 68.02587099271048\n",
      "loss at epoch 400: 65.38807540419106\n",
      "loss at epoch 500: 58.830339777760116\n",
      "loss at epoch 600: 54.22732667386771\n",
      "loss at epoch 700: 50.55093415573766\n",
      "loss at epoch 800: 47.74560321491536\n",
      "loss at epoch 900: 44.73411202441217\n",
      "loss at epoch 1000: 42.674532978222715\n",
      "loss at epoch 1100: 40.22851081431978\n",
      "loss at epoch 1200: 37.45028109530502\n",
      "loss at epoch 1300: 36.12487867914503\n",
      "loss at epoch 1400: 35.06549335818424\n",
      "loss at epoch 1500: 33.513962830639684\n",
      "loss at epoch 1600: 32.5512323935453\n",
      "loss at epoch 1700: 31.362170861993704\n",
      "loss at epoch 1800: 30.204606414217032\n",
      "loss at epoch 1900: 28.721560859253696\n",
      "loss at epoch 2000: 26.970357477770808\n",
      "loss at epoch 2100: 24.717921731469385\n",
      "loss at epoch 2200: 22.49272184873606\n",
      "loss at epoch 2300: 26.95544520791458\n",
      "loss at epoch 2400: 22.36715164290292\n",
      "loss at epoch 2500: 22.927457101045125\n",
      "loss at epoch 2600: 20.08979899166029\n",
      "loss at epoch 2700: 19.196828047924182\n",
      "loss at epoch 2800: 18.896647133355465\n",
      "loss at epoch 2900: 16.375881668641835\n",
      "loss at epoch 3000: 16.72261866189776\n",
      "loss at epoch 3100: 13.838755733258779\n",
      "loss at epoch 3200: 13.076008068052444\n",
      "loss at epoch 3300: 12.151538393305366\n",
      "loss at epoch 3400: 11.311482642558467\n",
      "loss at epoch 3500: 10.327645010343307\n",
      "loss at epoch 3600: 10.523296159596141\n",
      "loss at epoch 3700: 10.245959941644236\n",
      "loss at epoch 3800: 9.942483406298678\n",
      "loss at epoch 3900: 9.955048248908344\n",
      "loss at epoch 4000: 10.352242849777243\n",
      "loss at epoch 4100: 10.464099192444507\n",
      "loss at epoch 4200: 10.364028014593218\n",
      "loss at epoch 4300: 10.251650538093257\n",
      "loss at epoch 4400: 10.447283274950694\n",
      "loss at epoch 4500: 11.262703641141721\n",
      "loss at epoch 4600: 10.727188040921888\n",
      "loss at epoch 4700: 9.973869429813469\n",
      "loss at epoch 4800: 9.390145566791826\n",
      "loss at epoch 4900: 8.742774743879831\n"
     ]
    }
   ],
   "source": [
    "rnn2.train(5000, x, y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856fbaf1-78ac-47dc-8779-01f8619cae76",
   "metadata": {},
   "source": [
    "After training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "83380b2f-c662-4e2f-953d-4587a9530eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual: ['0022210110', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201220', '0002112100']\n",
      "predicted: ['0022210100', '2002002022', '1201102100', '1101201202', '0011102210', '2001012012', '1100100012', '2010122011', '2210201020', '0002112100']\n",
      "overlap: 98\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(rnn2, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54e994-07c9-4456-a1e9-e1c7573e2876",
   "metadata": {},
   "source": [
    "Pretty good! This concludes this note happily."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
