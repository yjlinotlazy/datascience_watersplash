{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442baf86-b098-48be-b917-9e1364cdaf69",
   "metadata": {},
   "source": [
    "# Recurrent Neural Net From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24d34a-4532-402b-a879-8ad5d6f54866",
   "metadata": {},
   "source": [
    "I will skip the reasoning behind why we need RNN. There's plenty on the internet. I'll focus on learning the technical side, and hope that the answers to \"why\"s will come naturally. This is the same principal as other notebooks in my \"from scratch\" series. This is heavily based on the basic neural net, so read that one first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975f819-9178-49bd-973e-0b66077089cb",
   "metadata": {},
   "source": [
    "Recall that in a neural network, we have:\n",
    "\n",
    "![nn](imgs/nn.png)\n",
    "\n",
    "For recurrent neural network, we carry over info from the previous data point in the sequence, by incorporating the values of the hidden layer, with its own weight $u$.\n",
    "\n",
    "![nn](imgs/recurrent_neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed60bf-8ff4-497f-a940-385ac635a0f6",
   "metadata": {},
   "source": [
    "Rephrasing the diagram, we have the feed forward process as\n",
    " * $s_h = xw + a_h^{prev}u + b_h$\n",
    " * $a_h = \\sigma_h(s_h)$\n",
    " * $s_o = a_hv +b_o$\n",
    " * $y = \\sigma_o(s_o)$\n",
    "\n",
    "h means hidden layer, o means output layer, s means weighted sum, a means activated value. $\\sigma$ means the activation function. For the activation functions, we use softmax at the output layer and tanh at the hidden layer.\n",
    "\n",
    "$$\\large\n",
    "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91459bbd-d455-44a8-a9f9-4cf45828c516",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "* Backpropagation from the output layer to the hidden layer remains the same since we don't add anything new between these two layers\n",
    "* Backpropagation from the hidden layer will go to both the input layer and the hidden layer of the previous data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e51d90-16d2-4dd5-b66c-2c9aa49a6541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
