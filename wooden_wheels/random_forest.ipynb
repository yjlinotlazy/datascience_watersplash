{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement random forest from scratch, start from decision treee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Decision tree is a machine learning method that makes sequential binary choices at\n",
    "each step based on one feature value per step, until a final value is assigned to the\n",
    "target variable.\n",
    "\n",
    "A decision tree on whether to date or not\n",
    "```\n",
    "        weather\n",
    "        /     \\\n",
    "      cold   hot\n",
    "      /         \\\n",
    "    location   time\n",
    "    /  \\        / \\\n",
    "   in  out  early  late\n",
    "  /      \\    /     \\\n",
    " Yes     No  No    Yes\n",
    "```\n",
    "\n",
    "It's called decision tree because one needs to transverse through a tree to\n",
    "make a final decision. Machine model comes in when figuring out how the tree should be\n",
    "constructed, i.e. what features to use and what values are used to split at each features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy & Where to split?\n",
    "\n",
    "At each step, ideally we split a heterogeneous pool into two homogeneous pools.\n",
    "To get close to that, we aim to reduce total entropy. This measure is also called\n",
    "information gain. We pick the value such that information gain is maximized when\n",
    "split optimally.\n",
    "\n",
    "Assuming binary classification, entropy is\n",
    "$$Entropy = -p_0\\log_2 p_0 - p_1\\log_2 p_1$$\n",
    "Where $p_0, p_1$ is the proprotion of samples in the two classes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def entropy(Y):\n",
    "    \"\"\"\n",
    "    sum of p0 log2(p0) + p1 log2(p1)\n",
    "    \"\"\"\n",
    "    total_ent = 0\n",
    "    size = len(Y)\n",
    "    p = sum([y == 1 for y in Y]) / size\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * math.log(p, 2) - (1 - p) * math.log(1 - p, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check entropy calculation. It should be like a bell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "samples = [[0] * x + [1] * (100 - x) for x in range(100)]\n",
    "proportions = [sum(s) / 100 for s in samples]\n",
    "entropies = [entropy(s) for s in samples]\n",
    "plt.plot()\n",
    "plt.scatter(proportions, entropies)\n",
    "plt.xlabel(\"Proportion of class 1\", fontsize=15)\n",
    "plt.ylabel(\"Entropy\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of how we can use entropy reduction to guide spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy before splitting: 1.0\n",
      "Entropy after a good split 0.7219280948873623\n",
      "Entropy after a bad split 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "before = [0] * 10 + [1] * 10\n",
    "print(\"Entropy before splitting:\", entropy(before))\n",
    "\n",
    "split1 = [0] * 8 + [1] *2\n",
    "split2 = [0] * 2 + [1] * 8\n",
    "entropy_good = entropy(split1) * len(split1) / len(before) \\\n",
    "    + entropy(split2) * len(split2) / len(before)\n",
    "print(\"Entropy after a good split\", entropy_good)\n",
    "\n",
    "split1 = [0] * 6 + [1] *4\n",
    "split2 = [0] * 4 + [1] * 6\n",
    "entropy_bad = entropy(split1) * len(split1) / len(before) \\\n",
    "    + entropy(split2) * len(split2) / len(before)\n",
    "print(\"Entropy after a bad split\", entropy_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any given feature, we want to find the split where entropy reduction is maximized.\n",
    "Iterate through all possible splitting point to do so.\n",
    "To find the best feature to split first, do an exaustive search for all splitting points\n",
    "of all features.\n",
    "\n",
    "First implement a method to find a optimal splitting point for one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_entropy(values, labels):\n",
    "    zipped = list(zip(values, labels))\n",
    "    zipped.sort(key=lambda x: x[0])\n",
    "    best_split, best_entr = values[0], entropy(labels)\n",
    "    size = len(labels)\n",
    "    for point in zipped:\n",
    "        # split samples based on split value\n",
    "        small_points = [p[1] for p in zipped if p[0] <= point[0]]\n",
    "        large_points = [p[1] for p in zipped if p[0] > point[0]]\n",
    "        if not small_points or not large_points:\n",
    "            continue\n",
    "        # calculate new entropy\n",
    "        sum_entropy = entropy(small_points) * len(small_points) / size\\\n",
    "            + entropy(large_points) * len(large_points) / size\n",
    "        # update best entropy if necessary\n",
    "        if sum_entropy < best_entr:\n",
    "            best_split, best_entr = point[0], sum_entropy\n",
    "    return best_split, best_entr\n",
    "\n",
    "# Example\n",
    "split_entropy([1, 2, 3, 4], [0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Nodes\n",
    "\n",
    "We use nodes to represent each decision point. The decision to branch left or\n",
    "right is determined by the feature value of the sample. For each node,\n",
    "we need to know which feature it's associated with and what value to use as the splitting\n",
    "point. For analytic purpose later, also make a `height` method to get height of the subtree\n",
    "rooted at the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.split = None\n",
    "        # left/right is either a node or a value. If it's a value then this node\n",
    "        # is a leaf\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    def move(self, val):\n",
    "        \"\"\"Move to left or right by comparing sample value to the splitting point\"\"\"\n",
    "        if val <= self.split:\n",
    "            return self.left\n",
    "        else:\n",
    "            return self.right\n",
    "    def height(self):\n",
    "        \"\"\"Get the height of the tree rooted at this node\"\"\"\n",
    "        if isinstance(self.left, float):\n",
    "            return 1\n",
    "        else:\n",
    "            return 1 + max([self.left.height(), self.right.height()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "A basic ID3 decision tree model is no more than a wrapper class that holds a tree. During\n",
    "training phase, it deterministically constructs nodes one by one, picking the best\n",
    "splitting value of the best feature at each iteration. This process is greedy and\n",
    "exaustive. To make a prediction, it transverse through the entire tree based on samples'\n",
    "values at each feature, until a leaf is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, df, features, target):\n",
    "        self.root = self._make_node(df, features, target)\n",
    "\n",
    "    def predict(self, df):\n",
    "        # transverse the tree for each data point\n",
    "        pred = df.apply(lambda row: self._transverse(self.root, row), axis=1)\n",
    "        return pred\n",
    "\n",
    "    def _best_feature(self, df, features, target):\n",
    "        \"\"\"\n",
    "        return the best feature given a set of feature values and a target.\n",
    "        Store everything in a dataframe and let user specify which feature\n",
    "        columns to use and which columns corresponds to the binary target\n",
    "        \"\"\"\n",
    "        best_split, best_entro, best_feature = None, None, None\n",
    "        # Try all features. At each iteration update the best option.\n",
    "        for feature in features:\n",
    "            this_split, this_entro = split_entropy(df[feature].tolist(), df[target])\n",
    "            if best_split is None or best_entro > this_entro:\n",
    "                best_split, best_entro, best_feature = this_split, this_entro, feature\n",
    "        return best_split, best_feature\n",
    "\n",
    "    def _make_node(self, df, features, target):\n",
    "        node = Node()\n",
    "        # Use the best splitting value of the best feature out of all.\n",
    "        node.split, node.feature = self._best_feature(df, features, target)\n",
    "        # Divide the samples into two subsets and use them to continue\n",
    "        # subtree construction\n",
    "        small_points = df[df[node.feature] <= node.split]\n",
    "        large_points = df[df[node.feature] > node.split]\n",
    "        # if No more split, return most frequent target value\n",
    "        if small_points.shape[0] == 0 or large_points.shape[0] == 0:\n",
    "            node.left = float(df[target].mode().iloc[0])\n",
    "            node.right = float(df[target].mode().iloc[0])\n",
    "        else:\n",
    "            node.left = self._make_node(small_points, features, target)\n",
    "            node.right = self._make_node(large_points, features, target)\n",
    "        return node\n",
    "\n",
    "    def _walk_node(self, node, row):\n",
    "        # Go to the next decision point or leaf\n",
    "        return node.move(row[node.feature])\n",
    "\n",
    "    def _transverse(self, node, row):\n",
    "        # Transverse nodes until a target (leaf) value is reached\n",
    "        next_location = self._walk_node(node, row)\n",
    "        while not isinstance(next_location, float):\n",
    "            next_location = self._walk_node(next_location, row)\n",
    "        return next_location\n",
    "\n",
    "    def depth(self):\n",
    "        return self.root.height()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plant some trees\n",
    "\n",
    "Use above simple tree to fit breast cancer data, a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data['data'])\n",
    "columns = [c for c in df.columns]\n",
    "df['target'] = data['target']\n",
    "# Count values to make sure accuracy makes sense (balanced class)\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the tree with the entire sample. This should create a HUGE tree that is 100% overfit and gives 100% accuracy, since the tree keeps making nodes until all samples are settled into homogeneous leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree()\n",
    "tree.fit(df, features=columns, target='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1.0\n",
      "Tree depth is 11\n"
     ]
    }
   ],
   "source": [
    "pred = tree.predict(df)\n",
    "acc = accuracy_score(y_pred=pred, y_true=df['target'])\n",
    "print(\"Accuracy is\", acc)\n",
    "print(\"Tree depth is\", tree.depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the tree with a subset of samples so that we can set aside test data. Compare performance\n",
    "on test vs training data to confirm overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.83\n",
      "Tree depth is 10\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree()\n",
    "train = df.head(200)\n",
    "test = df.tail(300)\n",
    "tree.fit(train, features=columns, target='target')\n",
    "pred = tree.predict(test)\n",
    "acc = accuracy_score(y_pred=pred, y_true=test['target'])\n",
    "print(\"Accuracy is\", acc)\n",
    "print(\"Tree depth is\", tree.depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcome overfitting\n",
    "\n",
    "There are many ways to avoid overfitting a tree, such as\n",
    " - Limit tree depth\n",
    " - Stop node construction once there are fewer than X number of samples in either split\n",
    " - Prune the tree later by removing leaves that do not contribute to prediction much\n",
    " - Use a subset of features\n",
    " - Fit on a subset of samples\n",
    " - Set a lower bound of information gain to continue node construction\n",
    " - Many more!\n",
    "\n",
    "Those are all trade-offs between bias and variation to avoid fitting on errors\n",
    "in the samples. Here's where a technique called `random forest` comes in. The general\n",
    "idea is that by creating a forest of trees and all of them to make prediction, then\n",
    "we mitigate overfitting. This is an ensemble method, where multiple estimators collectively\n",
    "make a decision on the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Random Forest Model\n",
    "\n",
    "To demonstrate that random forest enhances the power of decision tree, let's implement\n",
    "a classifier with 100 trees. Instead of fitting all samples with all features, each\n",
    "tree is fed with 66% of traning samples and 2/3 of the features. See what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleForest:\n",
    "    def __init__(self, n_trees=100):\n",
    "        self.trees = [DecisionTree() for _ in range(n_trees)]\n",
    "    \n",
    "    def fit(self, df, features, target):\n",
    "        for tree in self.trees:\n",
    "            subset = df.sample(frac=0.66)\n",
    "            sub_features = np.random.choice(features, int(2 * len(features) / 3))\n",
    "            tree.fit(subset, sub_features, target)\n",
    "    \n",
    "    def predict(self, df):\n",
    "        pred_list = [tree.predict(df) for tree in self.trees]\n",
    "        combined = pd.concat(pred_list, axis=1)\n",
    "        return combined.mode(axis=1).iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will take a few minutes due to inefficient implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = SimpleForest()\n",
    "forest.fit(train, features=columns, target='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = forest.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of random forest: 1.0\n",
      "Training accuracy of decision tree: 1.0\n",
      "Test accuracy of random forest: 0.93\n",
      "Test accuracy of decision tree: 0.83\n"
     ]
    }
   ],
   "source": [
    "pred = forest.predict(train)\n",
    "acc = accuracy_score(y_pred=pred, y_true=train['target'])\n",
    "\n",
    "pred_tree = tree.predict(train)\n",
    "tree_acc = accuracy_score(y_pred=pred_tree, y_true=train['target'])\n",
    "\n",
    "print(\"Training accuracy of random forest:\", acc)\n",
    "print(\"Training accuracy of decision tree:\", tree_acc)\n",
    "\n",
    "pred = forest.predict(test)\n",
    "acc = accuracy_score(y_pred=pred, y_true=test['target'])\n",
    "\n",
    "pred_tree = tree.predict(test)\n",
    "tree_acc = accuracy_score(y_pred=pred_tree, y_true=test['target'])\n",
    "\n",
    "print(\"Test accuracy of random forest:\", acc)\n",
    "print(\"Test accuracy of decision tree:\", tree_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is simple enough that random forest manages to get 100% accuracy on\n",
    "training samples and is still robust against unseen test samples. In most cases,\n",
    "it does not achieve 100% training accuracy, but that's not a problem and probably\n",
    "a good thing.\n",
    "\n",
    "Optimizations of decision tree can be used to random forest as well. Besides,\n",
    "random forest has additional optimization such as\n",
    " - Number of trees\n",
    " - Number of features to be sampled at each tree\n",
    " - How sampling is done for fitting each tree\n",
    " - Voting method at prediction time\n",
    " - Adding different weights to trees\n",
    " - More..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:th]",
   "language": "python",
   "name": "conda-env-th-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
